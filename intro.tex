Recently developed Large Multimodal Models (LMMs), exemplified by the latest GPT-4o \cite{openai2024gpt4o} and Gemini \cite{team2023gemini}, show promising capabilities of understanding both images and texts. However, even the latest LMMs often falter when required to perform multi-step mathematical reasoning in visual contexts \cite{}. Visual reasoning tasks require LMMs to interpret comprehensive mathematical implications from combined representations of visual entities, i.e., diagrams and tables, and natural languages, i.e., question content and execute stringent multi-step reasoning based on these learned representations simultaneously. This exposes a significant shortcoming in contemporary LMMs.

A plethora of visual question answering (VQA) datasets have been developed to evaluate the mathematical reasoning abilities of LMMs in visual contexts. Certain datasets including FigureQA \cite{kahou2017figureqa}, PlotQA \cite{methani2020plotqa}, ChartQA \cite{masry2022chartqa} have ventured into vision-language contexts for mathematical reasoning. However, most questions in the aforementioned VQA datasets focus on deriving answers from the visual objects, with only a small portion necessitating mathematical reasoning (illustrated in Figure \ref{xxx}). This leaves the in-depth assessment of current models' vision-language mathematical reasoning capabilities largely unexplored. Therefore, it is essential to develop a new benchmark to (1) facilitate the development of mathematical reasoning systems in visually intensive scenarios, and (2) evaluate the research progress of LLMs and LMMs, especially their capabilities in solving rigorous visual reasoning tasks.

In this paper, we choose to use math word problems (MWPs) with statistical charts to assess the multi-step mathematical reasoning abilities of LLMs and LMMs in visual contexts. A typical MWP with statistical charts is made up of (1) mixed question contents of natural languages, notations and equations and (2) a statistical chart that expresses mathematical quantities and relations. We give a real-world example in Figure \ref{xxx}. Successfully solving the MWPs with statistical charts require the models to not only understand every mathematical quantity and relation, but conduct rigorous multi-step reasoning. 
 
To facilitate research, we are releasing StatsChartMWP, a dataset of 8.5K high-quality MWPs with statistical charts from elementary to high school levels. The StatsChartMWP dataset contains a rich variety of 11 chart types, including bar graphs, line charts, and histograms, which provides opportunities to conduct fine-grained model performance analysis across different visual representations. Due to the space limit, we provide more StatsChartMWP questions samples from each category in Appendix \ref{xxx}. Every corresponding statistical chart is meticulously scanned to guarantee optimal clarity and accuracy. We have engineered this dataset to exhibit substantial linguistic and visual diversity, yet it is predicated on the relatively elementary principles of K-12 mathematics. Contemporary LLMs and LMMs encounter challenges in attaining superior performance on this dataset, predominantly due to the considerable diversity of the problems. However, the solutions offered by StatsChartMWP are exclusively dependent on K-12 concepts, making the attainment of high test performance a feasible objective.








The StatsChart dataset and its associated tasks introduce fresh avenues for research by presenting several technical hurdles: (1) the representation of innovative visual mediums of artificial figures such as diagrams, tables, and equations, (2) comprehension of technical language and equations, (3) the task of cross-modal alignment between figures and natural language, and (4) undertaking meticulous multi-step mathematical reasoning within visual contexts. Our human and quantitative studies reveal that current LMMs grapple with these challenges. To address the issue of weak multi-step multimodal mathematical reasoning abilities, we propose xxx, a multimodal xxx trained with a xxx. Despite xxx yielding some advancement, the StatsChart dataset continues to present unique challenges that promise to ignite future research in the fields of educational content modeling, multimodal reasoning, and question answering. to





To summarize, the contributions of this paper can be encapsulated as follows: (1) An exhaustive exploration into the prevailing challenges of current chart  benchmarks has been realized, leading to the creation of StatsChart, a comprehensive benchmark that encompasses 8,514 instances derived from authentic K12 mathematics education scenarios, each instance paired with its corresponding chart visual context. This benchmark covers educational stages from elementary through high school and is organized into 11 unique  chart categories. (2) By employing StatsChart, we have conducted a comparative assessment of several state-of-the-art LMMs. Our findings reveal a significant disparity in performance between open- and closed-source models, noting the GPT-4o model demonstrating pronounced superiority in terms of visual interpretation and reasoning capabilities. (3) Through a detailed taxonomy of statistical chart types, we have executed an in-depth error analysis pertaining to the large multimodal models. This scrutiny has been directed  at both the visual and reasoning dimensions, providing insightful directions for future model improvements.

=================

% 这段需要再检查，需要再更加确认实验后，增加一些实验的结论
A comprehensive series of experiments were carried out on the StatsChart platform to evaluate the efficacy of 12 foundational models in addressing statistical mathematical problems. Our investigation encompassed a spectrum of models, which included closed-source LMMs \citep{openai2024gpt4o, openai2023gpt4v, Qwen-VL}; open-source LMMs \citep{internvl-1.2, internvl-1.5, internlmxcomposer2, liu2024llavanext, lu2024deepseekvl, Qwen-VL, hpt1.0}; closed-source LLMs: \citep{openai2023gpt4} with visual models, wherein GPT-4V and GPT-4o specialized in image recognition. The findings from these experiments revealed a markedly superior performance of GPT-4o in comparison to its counterparts. 

\input{Table/datasets}


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{Figure/examples.pdf}
  \caption{Unique examples of our StatsChart benchmark. Note: The data image samples and associated textual prompts within the test set are provided in Chinese. To improve the readability of the article, these examples have been manually translated into English for demonstrative purposes.}
    \label{fig:examples}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{Figure/sub_questions_example.pdf}
  \caption{Examples presented here demonstrates a scenario characterized by sequential dependencies, as denoted by the green strikethrough. This annotation indicates that the resolution of sub-problem (1) is a prerequisite, even in instances where only sub-problem (2) is initially apparent.}
    \label{fig:sub_questions_example}
\end{figure}